{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                       \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                            \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                 \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                               \n",
       "// import $ivy.`sh.almond::almond-spark:0.6.0`\u001b[39m"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark:spark-core_2.12:2.4.3`\n",
    "import $ivy.`org.apache.spark:spark-sql_2.12:2.4.3`\n",
    "import $ivy.`org.apache.spark:spark-catalyst_2.12:2.4.3`\n",
    "import $ivy.`org.apache.spark:spark-streaming_2.12:2.4.3`\n",
    "import $ivy.`org.apache.spark:spark-sql-kafka-0-10_2.12:2.4.3`\n",
    "import $ivy.`io.delta:delta-core_2.12:0.3.0`\n",
    "// import $ivy.`sh.almond::almond-spark:0.6.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.Row\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mio.delta.tables._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.{Try,Success,Failure}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io.File\u001b[39m"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.Row\n",
    "import io.delta.tables._\n",
    "\n",
    "import scala.util.{Try,Success,Failure}\n",
    "import java.io.File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mRichDF\u001b[39m"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicit class RichDF(val ds: org.apache.spark.sql.DataFrame) {\n",
    "    def showHTML(limit:Int = 20, truncate: Int = 20) = {\n",
    "        import xml.Utility.escape\n",
    "        val data = ds.take(limit)\n",
    "        val header = ds.schema.fieldNames.toSeq        \n",
    "        val rows: Seq[Seq[String]] = data.map { row =>\n",
    "          row.toSeq.map { cell =>\n",
    "            val str = cell match {\n",
    "              case null => \"null\"\n",
    "              case binary: Array[Byte] => binary.map(\"%02X\".format(_)).mkString(\"[\", \" \", \"]\")\n",
    "              case array: Array[_] => array.mkString(\"[\", \", \", \"]\")\n",
    "              case seq: Seq[_] => seq.mkString(\"[\", \", \", \"]\")\n",
    "              case _ => cell.toString\n",
    "            }\n",
    "            if (truncate > 0 && str.length > truncate) {\n",
    "              // do not show ellipses for strings shorter than 4 characters.\n",
    "              if (truncate < 4) str.substring(0, truncate)\n",
    "              else str.substring(0, truncate - 3) + \"...\"\n",
    "            } else {\n",
    "              str\n",
    "            }\n",
    "          }: Seq[String]\n",
    "        }\n",
    "\n",
    "        publish.html(s\"\"\" <table>\n",
    "                <tr>\n",
    "                 ${header.map(h => s\"<th>${escape(h)}</th>\").mkString}\n",
    "                </tr>\n",
    "                ${rows.map { row =>\n",
    "                  s\"<tr>${row.map{c => s\"<td>${escape(c)}</td>\" }.mkString}</tr>\"\n",
    "                }.mkString}\n",
    "            </table>\n",
    "        \"\"\")        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Logger.getLogger(\"org\").setLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@3b10b47c\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "                .builder()\n",
    "                .config(\"spark.driver.memory\",\"5g\")\n",
    "                .master(\"local[*]\")\n",
    "                .getOrCreate()\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdirṔath\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"/home/vitor/Documents/datasets/events\"\u001b[39m"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dirṔath = \"/home/vitor/Documents/datasets/events\"\n",
    "\n",
    "if(!new File(dirṔath).exists()) {\n",
    "    val tempDF = spark.createDataFrame(spark.sparkContext.emptyRDD[Row],new StructType(\n",
    "        Array(\n",
    "            new StructField(\"id\",IntegerType),\n",
    "            new StructField(\"name\",StringType),\n",
    "            new StructField(\"maxPeopleAllowed\",IntegerType),\n",
    "            new StructField(\"eventType\",StringType),\n",
    "            new StructField(\"location\",StringType),\n",
    "            new StructField(\"lastUpdate\",TimestampType)\n",
    "        )\n",
    "    ))\n",
    "    tempDF.write.format(\"delta\").option(\"path\",\"/home/vitor/Documents/datasets/events\").save()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.sql.streaming.StreamingQueryException: A concurrent transaction has written new data since the current transaction read the table. Please try the operation again.\nConflicting commit: {\"version\":8,\"timestamp\":1567304058959,\"operation\":\"MERGE\",\"operationParameters\":{\"predicate\":((orig.`id` = updt.`id`) AND (orig.`lastUpdate` < updt.`lastUpdate`)),\"insertPredicate\":(updt.`id` IS NOT NULL)},\"readVersion\":7,\"isBlindAppend\":false}\nRefer to https://docs.delta.io/delta/isolation-level.html#optimistic-concurrency-control for more details.\n=== Streaming Query ===\nIdentifier: [id = b7475cfa-1af0-4bef-965e-2a38ade1fe51, runId = e1b4c226-f536-4686-904f-0dfd58efaaf5]\nCurrent Committed Offsets: {KafkaV2[Subscribe[animes_topic]]: {\"animes_topic\":{\"0\":15}}}\nCurrent Available Offsets: {KafkaV2[Subscribe[animes_topic]]: {\"animes_topic\":{\"0\":20}}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nProject [id#7799, name#7788, maxPeopleAllowed#7789, eventType#7790, location#7791, to_timestamp('lastUpdate, None) AS lastUpdate#7806]\n+- Project [cast(id#7787 as int) AS id#7799, name#7788, maxPeopleAllowed#7789, eventType#7790, location#7791, lastUpdate#7792]\n   +- Project [c0#7775 AS id#7787, c1#7776 AS name#7788, c2#7777 AS maxPeopleAllowed#7789, c3#7778 AS eventType#7790, c4#7779 AS location#7791, c5#7780 AS lastUpdate#7792]\n      +- Project [c0#7775, c1#7776, c2#7777, c3#7778, c4#7779, c5#7780]\n         +- Generate json_tuple(value#7773, id, name, maxPeopleAllowed, eventType, location, lastUpdate), false, [c0#7775, c1#7776, c2#7777, c3#7778, c4#7779, c5#7780]\n            +- Project [cast(value#7760 as string) AS value#7773]\n               +- StreamingExecutionRelation KafkaV2[Subscribe[animes_topic]], [key#7759, value#7760, topic#7761, partition#7762, offset#7763L, timestamp#7764, timestampType#7765]\n\u001b[39m\n  org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(\u001b[32mStreamExecution.scala\u001b[39m:\u001b[32m302\u001b[39m)\n  org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(\u001b[32mStreamExecution.scala\u001b[39m:\u001b[32m193\u001b[39m)\n\u001b[31morg.apache.spark.sql.delta.ConcurrentWriteException: A concurrent transaction has written new data since the current transaction read the table. Please try the operation again.\nConflicting commit: {\"version\":8,\"timestamp\":1567304058959,\"operation\":\"MERGE\",\"operationParameters\":{\"predicate\":((orig.`id` = updt.`id`) AND (orig.`lastUpdate` < updt.`lastUpdate`)),\"insertPredicate\":(updt.`id` IS NOT NULL)},\"readVersion\":7,\"isBlindAppend\":false}\nRefer to https://docs.delta.io/delta/isolation-level.html#optimistic-concurrency-control for more details.\u001b[39m\n  org.apache.spark.sql.delta.OptimisticTransactionImpl.$anonfun$checkAndRetry$2(\u001b[32mOptimisticTransaction.scala\u001b[39m:\u001b[32m443\u001b[39m)\n  scala.runtime.java8.JFunction1$mcVJ$sp.apply(\u001b[32mJFunction1$mcVJ$sp.java\u001b[39m:\u001b[32m23\u001b[39m)\n  scala.collection.immutable.NumericRange.foreach(\u001b[32mNumericRange.scala\u001b[39m:\u001b[32m74\u001b[39m)\n  org.apache.spark.sql.delta.OptimisticTransactionImpl.$anonfun$checkAndRetry$1(\u001b[32mOptimisticTransaction.scala\u001b[39m:\u001b[32m416\u001b[39m)\n  scala.runtime.java8.JFunction0$mcJ$sp.apply(\u001b[32mJFunction0$mcJ$sp.java\u001b[39m:\u001b[32m23\u001b[39m)\n  com.databricks.spark.util.DatabricksLogging.recordOperation(\u001b[32mDatabricksLogging.scala\u001b[39m:\u001b[32m77\u001b[39m)\n  com.databricks.spark.util.DatabricksLogging.recordOperation$(\u001b[32mDatabricksLogging.scala\u001b[39m:\u001b[32m67\u001b[39m)\n  org.apache.spark.sql.delta.OptimisticTransaction.recordOperation(\u001b[32mOptimisticTransaction.scala\u001b[39m:\u001b[32m77\u001b[39m)\n  org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(\u001b[32mDeltaLogging.scala\u001b[39m:\u001b[32m103\u001b[39m)\n  org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(\u001b[32mDeltaLogging.scala\u001b[39m:\u001b[32m89\u001b[39m)\n  org.apache.spark.sql.delta.OptimisticTransaction.recordDeltaOperation(\u001b[32mOptimisticTransaction.scala\u001b[39m:\u001b[32m77\u001b[39m)\n  org.apache.spark.sql.delta.OptimisticTransactionImpl.checkAndRetry(\u001b[32mOptimisticTransaction.scala\u001b[39m:\u001b[32m412\u001b[39m)\n  org.apache.spark.sql.delta.OptimisticTransactionImpl.checkAndRetry$(\u001b[32mOptimisticTransaction.scala\u001b[39m:\u001b[32m406\u001b[39m)\n  org.apache.spark.sql.delta.OptimisticTransaction.checkAndRetry(\u001b[32mOptimisticTransaction.scala\u001b[39m:\u001b[32m77\u001b[39m)\n  org.apache.spark.sql.delta.OptimisticTransactionImpl.$anonfun$doCommit$1(\u001b[32mOptimisticTransaction.scala\u001b[39m:\u001b[32m397\u001b[39m)\n  scala.runtime.java8.JFunction0$mcJ$sp.apply(\u001b[32mJFunction0$mcJ$sp.java\u001b[39m:\u001b[32m23\u001b[39m)\n  org.apache.spark.sql.delta.DeltaLog.lockInterruptibly(\u001b[32mDeltaLog.scala\u001b[39m:\u001b[32m200\u001b[39m)\n  org.apache.spark.sql.delta.OptimisticTransactionImpl.doCommit(\u001b[32mOptimisticTransaction.scala\u001b[39m:\u001b[32m350\u001b[39m)\n  org.apache.spark.sql.delta.OptimisticTransactionImpl.$anonfun$commit$1(\u001b[32mOptimisticTransaction.scala\u001b[39m:\u001b[32m264\u001b[39m)\n  scala.runtime.java8.JFunction0$mcJ$sp.apply(\u001b[32mJFunction0$mcJ$sp.java\u001b[39m:\u001b[32m23\u001b[39m)\n  com.databricks.spark.util.DatabricksLogging.recordOperation(\u001b[32mDatabricksLogging.scala\u001b[39m:\u001b[32m77\u001b[39m)\n  com.databricks.spark.util.DatabricksLogging.recordOperation$(\u001b[32mDatabricksLogging.scala\u001b[39m:\u001b[32m67\u001b[39m)\n  org.apache.spark.sql.delta.OptimisticTransaction.recordOperation(\u001b[32mOptimisticTransaction.scala\u001b[39m:\u001b[32m77\u001b[39m)\n  org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(\u001b[32mDeltaLogging.scala\u001b[39m:\u001b[32m103\u001b[39m)\n  org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(\u001b[32mDeltaLogging.scala\u001b[39m:\u001b[32m89\u001b[39m)\n  org.apache.spark.sql.delta.OptimisticTransaction.recordDeltaOperation(\u001b[32mOptimisticTransaction.scala\u001b[39m:\u001b[32m77\u001b[39m)\n  org.apache.spark.sql.delta.OptimisticTransactionImpl.commit(\u001b[32mOptimisticTransaction.scala\u001b[39m:\u001b[32m241\u001b[39m)\n  org.apache.spark.sql.delta.OptimisticTransactionImpl.commit$(\u001b[32mOptimisticTransaction.scala\u001b[39m:\u001b[32m239\u001b[39m)\n  org.apache.spark.sql.delta.OptimisticTransaction.commit(\u001b[32mOptimisticTransaction.scala\u001b[39m:\u001b[32m77\u001b[39m)\n  org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$run$2(\u001b[32mMergeIntoCommand.scala\u001b[39m:\u001b[32m137\u001b[39m)\n  org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$run$2$adapted(\u001b[32mMergeIntoCommand.scala\u001b[39m:\u001b[32m121\u001b[39m)\n  org.apache.spark.sql.delta.DeltaLog.withNewTransaction(\u001b[32mDeltaLog.scala\u001b[39m:\u001b[32m386\u001b[39m)\n  org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$run$1(\u001b[32mMergeIntoCommand.scala\u001b[39m:\u001b[32m121\u001b[39m)\n  com.databricks.spark.util.DatabricksLogging.recordOperation(\u001b[32mDatabricksLogging.scala\u001b[39m:\u001b[32m77\u001b[39m)\n  com.databricks.spark.util.DatabricksLogging.recordOperation$(\u001b[32mDatabricksLogging.scala\u001b[39m:\u001b[32m67\u001b[39m)\n  org.apache.spark.sql.delta.commands.MergeIntoCommand.recordOperation(\u001b[32mMergeIntoCommand.scala\u001b[39m:\u001b[32m88\u001b[39m)\n  org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(\u001b[32mDeltaLogging.scala\u001b[39m:\u001b[32m103\u001b[39m)\n  org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(\u001b[32mDeltaLogging.scala\u001b[39m:\u001b[32m89\u001b[39m)\n  org.apache.spark.sql.delta.commands.MergeIntoCommand.recordDeltaOperation(\u001b[32mMergeIntoCommand.scala\u001b[39m:\u001b[32m88\u001b[39m)\n  org.apache.spark.sql.delta.commands.MergeIntoCommand.run(\u001b[32mMergeIntoCommand.scala\u001b[39m:\u001b[32m120\u001b[39m)\n  io.delta.tables.DeltaMergeBuilder.execute(\u001b[32mDeltaMergeBuilder.scala\u001b[39m:\u001b[32m233\u001b[39m)\n  ammonite.$sess.cmd55$Helper.$anonfun$res55_4$1(\u001b[32mcmd55.sc\u001b[39m:\u001b[32m28\u001b[39m)\n  ammonite.$sess.cmd55$Helper.$anonfun$res55_4$1$adapted(\u001b[32mcmd55.sc\u001b[39m:\u001b[32m20\u001b[39m)\n  org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(\u001b[32mForeachBatchSink.scala\u001b[39m:\u001b[32m35\u001b[39m)\n  org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$15(\u001b[32mMicroBatchExecution.scala\u001b[39m:\u001b[32m537\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m78\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m125\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m73\u001b[39m)\n  org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$14(\u001b[32mMicroBatchExecution.scala\u001b[39m:\u001b[32m536\u001b[39m)\n  org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(\u001b[32mProgressReporter.scala\u001b[39m:\u001b[32m351\u001b[39m)\n  org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(\u001b[32mProgressReporter.scala\u001b[39m:\u001b[32m349\u001b[39m)\n  org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(\u001b[32mStreamExecution.scala\u001b[39m:\u001b[32m58\u001b[39m)\n  org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(\u001b[32mMicroBatchExecution.scala\u001b[39m:\u001b[32m535\u001b[39m)\n  org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(\u001b[32mMicroBatchExecution.scala\u001b[39m:\u001b[32m198\u001b[39m)\n  scala.runtime.java8.JFunction0$mcV$sp.apply(\u001b[32mJFunction0$mcV$sp.java\u001b[39m:\u001b[32m23\u001b[39m)\n  org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(\u001b[32mProgressReporter.scala\u001b[39m:\u001b[32m351\u001b[39m)\n  org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(\u001b[32mProgressReporter.scala\u001b[39m:\u001b[32m349\u001b[39m)\n  org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(\u001b[32mStreamExecution.scala\u001b[39m:\u001b[32m58\u001b[39m)\n  org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(\u001b[32mMicroBatchExecution.scala\u001b[39m:\u001b[32m166\u001b[39m)\n  org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(\u001b[32mTriggerExecutor.scala\u001b[39m:\u001b[32m56\u001b[39m)\n  org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(\u001b[32mMicroBatchExecution.scala\u001b[39m:\u001b[32m160\u001b[39m)\n  org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(\u001b[32mStreamExecution.scala\u001b[39m:\u001b[32m281\u001b[39m)\n  org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(\u001b[32mStreamExecution.scala\u001b[39m:\u001b[32m193\u001b[39m)"
     ]
    }
   ],
   "source": [
    "val columns = Seq(\"id\",\"name\",\"maxPeopleAllowed\",\"eventType\",\"location\",\"lastUpdate\")\n",
    "val delta = DeltaTable.forPath(dirṔath)\n",
    "val stream = spark\n",
    "                .readStream\n",
    "                .format(\"kafka\")\n",
    "                .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "                .option(\"subscribe\", \"animes_topic\")\n",
    "                .load()\n",
    "\n",
    "val parsedDF = stream\n",
    "                .selectExpr(\"cast(value as String) as value\")\n",
    "                .select(json_tuple($\"value\",columns:_*))\n",
    "                .toDF(columns:_*)\n",
    "                .withColumn(\"id\", $\"id\".cast(IntegerType))\n",
    "                .withColumn(\"lastUpdate\",to_timestamp($\"lastUpdate\"))\n",
    "parsedDF\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"update\")\n",
    "    .foreachBatch((microBatchOutputDF: org.apache.spark.sql.DataFrame, batchId: Long) => {\n",
    "        \n",
    "         delta.as(\"orig\")\n",
    "            .merge(\n",
    "                microBatchOutputDF.as(\"updt\"),\n",
    "                \"orig.id = updt.id and orig.lastUpdate < updt.lastUpdate\"\n",
    "            )\n",
    "            .whenMatched().updateAll()\n",
    "            .whenNotMatched(\"updt.id is not null\").insertAll()\n",
    "            .execute()        \n",
    "\n",
    "    }).start().awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

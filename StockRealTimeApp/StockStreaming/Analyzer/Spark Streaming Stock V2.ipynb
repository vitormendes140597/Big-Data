{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                       \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                            \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                      \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                               \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                   \u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark:spark-core_2.11:2.4.4`\n",
    "import $ivy.`org.apache.spark:spark-sql_2.11:2.4.4`\n",
    "import $ivy.`org.apache.spark:spark-streaming_2.11:2.4.4`\n",
    "import $ivy.`com.microsoft.azure:azure-eventhubs-spark_2.11:2.3.13`\n",
    "import $ivy.`io.delta:delta-core_2.11:0.3.0`\n",
    "import $ivy.`log4j:log4j:1.2.17`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkConf\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\n",
       "\n",
       "// Event Hubs\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.eventhubs.{EventHubsConf,EventPosition}\n",
       "\n",
       "// Delta Lake\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mio.delta.tables.DeltaTable\n",
       "\n",
       "// Log4j\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level,Logger}\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Spark\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "// Event Hubs\n",
    "import org.apache.spark.eventhubs.{EventHubsConf,EventPosition}\n",
    "\n",
    "// Delta Lake\n",
    "import io.delta.tables.DeltaTable\n",
    "\n",
    "// Log4j\n",
    "import org.apache.log4j.{Level,Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "Logger.getLogger(\"akka\").setLevel(Level.OFF)\n",
    "Logger.getRootLogger.setLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkConfig\u001b[39m: \u001b[32mSparkConf\u001b[39m = org.apache.spark.SparkConf@49ee418b\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@65788322"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Criando Contexto de Spark\n",
    "val sparkConfig = new SparkConf().setAll(Seq((\"spark.driver.memory\",\"2g\")))\n",
    "val spark = SparkSession.builder.config(sparkConfig).master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions._\u001b[39m"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.expressions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mkey\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"Endpoint=sb://stockhub.servicebus.windows.net/;SharedAccessKeyName=listen;SharedAccessKey=q/ZEO4MaguioHiH2vQof02xdsGZVGE1V5RrE9IldOWU=;EntityPath=ev-stock\"\u001b[39m\n",
       "\u001b[36mevConf\u001b[39m: \u001b[32mEventHubsConf\u001b[39m = org.apache.spark.eventhubs.EventHubsConf@1d9d998c"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Criando Conexão com o Event Hubs\n",
    "val key =  \"Endpoint=sb://stockhub.servicebus.windows.net/;SharedAccessKeyName=listen;SharedAccessKey=q/ZEO4MaguioHiH2vQof02xdsGZVGE1V5RrE9IldOWU=;EntityPath=ev-stock\"\n",
    "val evConf = EventHubsConf(key).setStartingPosition(EventPosition.fromStartOfStream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mrawDF\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [body: binary, partition: string ... 7 more fields]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawDF = spark.read.format(\"eventhubs\").options(evConf.toMap).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- symbol: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- price_open: string (nullable = true)\n",
      " |-- day_high: string (nullable = true)\n",
      " |-- day_low: string (nullable = true)\n",
      " |-- 52_week_low: string (nullable = true)\n",
      " |-- day_change: string (nullable = true)\n",
      " |-- change_pct: string (nullable = true)\n",
      " |-- close_yesterday: string (nullable = true)\n",
      " |-- market_cap: string (nullable = true)\n",
      " |-- volume: string (nullable = true)\n",
      " |-- volume_avg: string (nullable = true)\n",
      " |-- shares: string (nullable = true)\n",
      " |-- timezone_name: string (nullable = true)\n",
      " |-- gmt_offset: string (nullable = true)\n",
      " |-- last_trade_time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rawDF\n",
    "    .select($\"body\".cast(StringType))\n",
    "    .select(get_json_object($\"body\",\"$.data\") as \"data\")\n",
    "    .select(parseMultipleJsons($\"data\") as \"data\")\n",
    "    .select(explode($\"data\") as \"data\")\n",
    "    .select(json_tuple($\"data\",columns:_*))\n",
    "    .toDF(columns:_*)\n",
    "    .printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mparseMultipleJsons\u001b[39m: \u001b[32mUserDefinedFunction\u001b[39m = \u001b[33mUserDefinedFunction\u001b[39m(\n",
       "  <function1>,\n",
       "  \u001b[33mArrayType\u001b[39m(StringType, true),\n",
       "  \u001b[33mSome\u001b[39m(\u001b[33mList\u001b[39m(StringType))\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Para cada linha da nossa tabela, deixa em um formato mais fácil de manipular\n",
    "val parseMultipleJsons = udf((json : String) => {\n",
    "    val jsonSet = \"[]\".toSet\n",
    "    \n",
    "    json\n",
    "        .filterNot(jsonSet)\n",
    "        .replaceAll(\"},\",\"}-,},\")\n",
    "        .split(\"},\")\n",
    "        .map(r => r.replaceAll(\"}-,\",\"}\").trim)\n",
    "        .filter(r => r.length > 0)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mparseStreamDF\u001b[39m"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parseStreamDF(df : org.apache.spark.sql.DataFrame) : org.apache.spark.sql.DataFrame = {\n",
    "    df\n",
    "        .select($\"body\".cast(StringType))\n",
    "        .select(get_json_object($\"body\",\"$.data\") as \"data\")\n",
    "        .select(parseMultipleJsons($\"data\") as \"data\")\n",
    "        .select(explode($\"data\") as \"data\")\n",
    "        .select(json_tuple($\"data\",columns:_*))\n",
    "        .toDF(columns:_*)\n",
    "        .withColumn(\"price\",$\"price\".cast(DoubleType))\n",
    "        .withColumn(\"day_high\",$\"day_high\".cast(DoubleType))\n",
    "        .withColumn(\"day_low\",$\"day_high\".cast(DoubleType))\n",
    "        .withColumn(\"52_week_low\",$\"52_week_low\".cast(DoubleType))\n",
    "        .withColumn(\"day_change\",$\"day_change\".cast(DoubleType))\n",
    "        .withColumn(\"change_pct\",$\"change_pct\".cast(DoubleType))\n",
    "        .withColumn(\"close_yesterday\",$\"close_yesterday\".cast(DoubleType))\n",
    "        .withColumn(\"market_cap\",$\"market_cap\".cast(LongType))\n",
    "        .withColumn(\"volume\",$\"volume\".cast(LongType))\n",
    "        .withColumn(\"volume_avg\",$\"volume_avg\".cast(DoubleType))\n",
    "        .withColumn(\"shares\",$\"shares\".cast(LongType))\n",
    "        .withColumn(\"gmt_offset\",$\"shares\".cast(IntegerType))\n",
    "        .withColumn(\"last_trade_time\",to_timestamp($\"last_trade_time\"))\n",
    "        .withColumn(\"brazil_time_zone\",from_utc_timestamp(to_utc_timestamp($\"last_trade_time\",\"America/New_York\"),\"America/Sao_Paulo\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mcolumns\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m\"symbol\"\u001b[39m,\n",
       "  \u001b[32m\"name\"\u001b[39m,\n",
       "  \u001b[32m\"currency\"\u001b[39m,\n",
       "  \u001b[32m\"price\"\u001b[39m,\n",
       "  \u001b[32m\"price_open\"\u001b[39m,\n",
       "  \u001b[32m\"day_high\"\u001b[39m,\n",
       "  \u001b[32m\"day_low\"\u001b[39m,\n",
       "  \u001b[32m\"52_week_low\"\u001b[39m,\n",
       "  \u001b[32m\"day_change\"\u001b[39m,\n",
       "  \u001b[32m\"change_pct\"\u001b[39m,\n",
       "  \u001b[32m\"close_yesterday\"\u001b[39m,\n",
       "  \u001b[32m\"market_cap\"\u001b[39m,\n",
       "  \u001b[32m\"volume\"\u001b[39m,\n",
       "  \u001b[32m\"volume_avg\"\u001b[39m,\n",
       "  \u001b[32m\"shares\"\u001b[39m,\n",
       "  \u001b[32m\"timezone_name\"\u001b[39m,\n",
       "  \u001b[32m\"gmt_offset\"\u001b[39m,\n",
       "  \u001b[32m\"last_trade_time\"\u001b[39m\n",
       ")\n",
       "\u001b[36mstockDF2\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [symbol: string, name: string ... 17 more fields]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val columns = Array(\"symbol\",\"name\",\"currency\",\"price\",\"price_open\",\"day_high\",\"day_low\",\"52_week_low\",\"day_change\",\"change_pct\",\n",
    "                 \"close_yesterday\",\"market_cap\",\"volume\",\"volume_avg\",\"shares\",\"timezone_name\",\"gmt_offset\",\"last_trade_time\")\n",
    "val stockDF2 = parseStreamDF(rawDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mstockDataPath\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"/home/vitor/Documents/Big-Data/StockStreaming/Analyzer/stockdataV3\"\u001b[39m"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stockDataPath = \"/home/vitor/Documents/Big-Data/StockStreaming/Analyzer/stockdataV3\"\n",
    "\n",
    "stockDF2\n",
    "    .withColumn(\"row\",row_number().over(Window.partitionBy($\"symbol\").orderBy($\"last_trade_time\".desc)))\n",
    "    .where(\"row = 1\")\n",
    "    .drop(\"row\")\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .option(\"path\",stockDataPath)\n",
    "//     .save(\"/home/vitor/Documents/Big-Data/StockStreaming/Analyzer/stockdataV2\")\n",
    "    .saveAsTable(\"stockV3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       " <table>\n",
       "                <tr>\n",
       "                 <th>symbol</th><th>name</th><th>currency</th><th>price</th><th>price_open</th><th>day_high</th><th>day_low</th><th>52_week_low</th><th>day_change</th><th>change_pct</th><th>close_yesterday</th><th>market_cap</th><th>volume</th><th>volume_avg</th><th>shares</th><th>timezone_name</th><th>gmt_offset</th><th>last_trade_time</th><th>brazil_time_zone</th>\n",
       "                </tr>\n",
       "                <tr><td>NVDA</td><td>NVIDIA Corporation</td><td>USD</td><td>172.53</td><td>176.10</td><td>177.29</td><td>177.29</td><td>124.46</td><td>-2.31</td><td>-1.32</td><td>174.84</td><td>105070772224</td><td>7442704</td><td>6493600.0</td><td>609000000</td><td>America/New_York</td><td>609000000</td><td>2019-09-24 16:00:...</td><td>2019-09-24 17:00:...</td></tr><tr><td>AMZN</td><td>Amazon.com, Inc.</td><td>USD</td><td>1741.61</td><td>1790.60</td><td>1795.7</td><td>1795.7</td><td>1307.0</td><td>-43.69</td><td>-2.45</td><td>1785.3</td><td>861497851904</td><td>4315284</td><td>3076533.0</td><td>494656000</td><td>America/New_York</td><td>494656000</td><td>2019-09-24 16:00:...</td><td>2019-09-24 17:00:...</td></tr><tr><td>GOOG</td><td>Alphabet Inc.</td><td>BRL</td><td>1218.76</td><td>1240.00</td><td>1246.56</td><td>1246.56</td><td>970.11</td><td>-15.27</td><td>-1.24</td><td>1234.03</td><td>845311180800</td><td>1372965</td><td>1245316.0</td><td>347344992</td><td>America/New_York</td><td>347344992</td><td>2019-09-24 16:00:...</td><td>2019-09-24 17:00:...</td></tr><tr><td>FB</td><td>Facebook, Inc.</td><td>USD</td><td>181.28</td><td>187.98</td><td>188.04</td><td>188.04</td><td>123.02</td><td>-5.54</td><td>-2.97</td><td>186.82</td><td>517182783488</td><td>15753694</td><td>1.1908716E7</td><td>2405720064</td><td>America/New_York</td><td>-1889247232</td><td>2019-09-24 16:00:...</td><td>2019-09-24 17:00:...</td></tr>\n",
       "            </table>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.read.table(\"stockV3\").showHTML()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       " <table>\n",
       "                <tr>\n",
       "                 <th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th>\n",
       "                </tr>\n",
       "                <tr><td>2</td><td>2019-09-24 18:48:...</td><td>null</td><td>null</td><td>DELETE</td><td>Map(predicate -&gt; ...</td><td>null</td><td>null</td><td>null</td><td>1</td><td>null</td><td>false</td></tr><tr><td>1</td><td>2019-09-24 18:40:...</td><td>null</td><td>null</td><td>UPDATE</td><td>Map(predicate -&gt; ...</td><td>null</td><td>null</td><td>null</td><td>0</td><td>null</td><td>false</td></tr><tr><td>0</td><td>2019-09-24 18:36:...</td><td>null</td><td>null</td><td>WRITE</td><td>Map(mode -&gt; Overw...</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>false</td></tr>\n",
       "            </table>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Recuperando tabela Delta\n",
    "DeltaTable\n",
    "    .forPath(stockDataPath)\n",
    "    .history\n",
    "    .showHTML()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       " <table>\n",
       "                <tr>\n",
       "                 <th>symbol</th><th>name</th><th>currency</th><th>price</th><th>price_open</th><th>day_high</th><th>day_low</th><th>52_week_low</th><th>day_change</th><th>change_pct</th><th>close_yesterday</th><th>market_cap</th><th>volume</th><th>volume_avg</th><th>shares</th><th>timezone_name</th><th>gmt_offset</th><th>last_trade_time</th><th>brazil_time_zone</th>\n",
       "                </tr>\n",
       "                <tr><td>NVDA</td><td>NVIDIA Corporation</td><td>USD</td><td>172.53</td><td>176.10</td><td>177.29</td><td>177.29</td><td>124.46</td><td>-2.31</td><td>-1.32</td><td>174.84</td><td>105070772224</td><td>7442704</td><td>6493600.0</td><td>609000000</td><td>America/New_York</td><td>609000000</td><td>2019-09-24 16:00:...</td><td>2019-09-24 17:00:...</td></tr><tr><td>AMZN</td><td>Amazon.com, Inc.</td><td>USD</td><td>1741.61</td><td>1790.60</td><td>1795.7</td><td>1795.7</td><td>1307.0</td><td>-43.69</td><td>-2.45</td><td>1785.3</td><td>861497851904</td><td>4315284</td><td>3076533.0</td><td>494656000</td><td>America/New_York</td><td>494656000</td><td>2019-09-24 16:00:...</td><td>2019-09-24 17:00:...</td></tr><tr><td>GOOG</td><td>Alphabet Inc.</td><td>USD</td><td>1218.76</td><td>1240.00</td><td>1246.56</td><td>1246.56</td><td>970.11</td><td>-15.27</td><td>-1.24</td><td>1234.03</td><td>845311180800</td><td>1372965</td><td>1245316.0</td><td>347344992</td><td>America/New_York</td><td>347344992</td><td>2019-09-24 16:00:...</td><td>2019-09-24 17:00:...</td></tr><tr><td>FB</td><td>Facebook, Inc.</td><td>USD</td><td>181.28</td><td>187.98</td><td>188.04</td><td>188.04</td><td>123.02</td><td>-5.54</td><td>-2.97</td><td>186.82</td><td>517182783488</td><td>15753694</td><td>1.1908716E7</td><td>2405720064</td><td>America/New_York</td><td>-1889247232</td><td>2019-09-24 16:00:...</td><td>2019-09-24 17:00:...</td></tr>\n",
       "            </table>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.read.format(\"delta\").option(\"versionAsOf\",\"0\").load(stockDataPath).showHTML()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdeltaStock\u001b[39m: \u001b[32mDeltaTable\u001b[39m = io.delta.tables.DeltaTable@78c08041"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Realizando Update na tabela delta\n",
    "val deltaStock = DeltaTable\n",
    "                    .forPath(stockDataPath)\n",
    "\n",
    "deltaStock.updateExpr(\n",
    "    \"symbol = 'GOOG'\", Map(\"currency\" -> \"'BRL'\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Deletando um registro na tabela delta\n",
    "deltaStock.delete(\"symbol = 'NVDA'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       " <table>\n",
       "                <tr>\n",
       "                 <th>symbol</th><th>name</th><th>currency</th><th>price</th><th>price_open</th><th>day_high</th><th>day_low</th><th>52_week_low</th><th>day_change</th><th>change_pct</th><th>close_yesterday</th><th>market_cap</th><th>volume</th><th>volume_avg</th><th>shares</th><th>timezone_name</th><th>gmt_offset</th><th>last_trade_time</th><th>brazil_time_zone</th>\n",
       "                </tr>\n",
       "                <tr><td>AMZN</td><td>Amazon.com, Inc.</td><td>USD</td><td>1741.61</td><td>1790.60</td><td>1795.7</td><td>1795.7</td><td>1307.0</td><td>-43.69</td><td>-2.45</td><td>1785.3</td><td>861497851904</td><td>4315284</td><td>3076533.0</td><td>494656000</td><td>America/New_York</td><td>494656000</td><td>2019-09-24 16:00:...</td><td>2019-09-24 17:00:...</td></tr><tr><td>GOOG</td><td>Alphabet Inc.</td><td>BRL</td><td>1218.76</td><td>1240.00</td><td>1246.56</td><td>1246.56</td><td>970.11</td><td>-15.27</td><td>-1.24</td><td>1234.03</td><td>845311180800</td><td>1372965</td><td>1245316.0</td><td>347344992</td><td>America/New_York</td><td>347344992</td><td>2019-09-24 16:00:...</td><td>2019-09-24 17:00:...</td></tr><tr><td>FB</td><td>Facebook, Inc.</td><td>USD</td><td>181.28</td><td>187.98</td><td>188.04</td><td>188.04</td><td>123.02</td><td>-5.54</td><td>-2.97</td><td>186.82</td><td>517182783488</td><td>15753694</td><td>1.1908716E7</td><td>2405720064</td><td>America/New_York</td><td>-1889247232</td><td>2019-09-24 16:00:...</td><td>2019-09-24 17:00:...</td></tr>\n",
       "            </table>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deltaStock.toDF.showHTML()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       " <table>\n",
       "                <tr>\n",
       "                 <th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th>\n",
       "                </tr>\n",
       "                <tr><td>3</td><td>2019-09-22 18:28:...</td><td>null</td><td>null</td><td>WRITE</td><td>Map(mode -&gt; Overw...</td><td>null</td><td>null</td><td>null</td><td>2</td><td>null</td><td>false</td></tr><tr><td>2</td><td>2019-09-22 18:23:...</td><td>null</td><td>null</td><td>DELETE</td><td>Map(predicate -&gt; ...</td><td>null</td><td>null</td><td>null</td><td>1</td><td>null</td><td>false</td></tr><tr><td>1</td><td>2019-09-22 18:22:...</td><td>null</td><td>null</td><td>UPDATE</td><td>Map(predicate -&gt; ...</td><td>null</td><td>null</td><td>null</td><td>0</td><td>null</td><td>false</td></tr><tr><td>0</td><td>2019-09-22 18:18:...</td><td>null</td><td>null</td><td>WRITE</td><td>Map(mode -&gt; Overw...</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>false</td></tr>\n",
       "            </table>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deltaStock.history.showHTML()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lendo Dados do Event Hub em Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mcheckpoint\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"/home/vitor/Documents/Big-Data/StockStreaming/Analyzer/stockCheckpointV2\"\u001b[39m\n",
       "\u001b[36mrawStreamDF\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [body: binary, partition: string ... 7 more fields]\n",
       "\u001b[36mparsedStreamDF\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [symbol: string, name: string ... 17 more fields]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val checkpoint = \"/home/vitor/Documents/Big-Data/StockStreaming/Analyzer/stockCheckpointV2\"\n",
    "val rawStreamDF = spark.readStream.format(\"eventhubs\").options(evConf.toMap).load()\n",
    "val parsedStreamDF = parseStreamDF(rawStreamDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Interrupted!\n  sun.misc.Unsafe.park(\u001b[32mNative Method\u001b[39m)\n  java.util.concurrent.locks.LockSupport.park(\u001b[32mLockSupport.java\u001b[39m:\u001b[32m175\u001b[39m)\n  java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(\u001b[32mAbstractQueuedSynchronizer.java\u001b[39m:\u001b[32m836\u001b[39m)\n  java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(\u001b[32mAbstractQueuedSynchronizer.java\u001b[39m:\u001b[32m997\u001b[39m)\n  java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(\u001b[32mAbstractQueuedSynchronizer.java\u001b[39m:\u001b[32m1304\u001b[39m)\n  java.util.concurrent.CountDownLatch.await(\u001b[32mCountDownLatch.java\u001b[39m:\u001b[32m231\u001b[39m)\n  org.apache.spark.sql.execution.streaming.StreamExecution.awaitTermination(\u001b[32mStreamExecution.scala\u001b[39m:\u001b[32m469\u001b[39m)\n  org.apache.spark.sql.execution.streaming.StreamingQueryWrapper.awaitTermination(\u001b[32mStreamingQueryWrapper.scala\u001b[39m:\u001b[32m53\u001b[39m)\n  ammonite.$sess.cmd51$Helper.<init>(\u001b[32mcmd51.sc\u001b[39m:\u001b[32m21\u001b[39m)\n  ammonite.$sess.cmd51$.<init>(\u001b[32mcmd51.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd51$.<clinit>(\u001b[32mcmd51.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "parsedStreamDF\n",
    "    .writeStream\n",
    "    .option(\"checkpoinLocation\",checkpoint)\n",
    "    .foreachBatch((microBatchOutputDF : org.apache.spark.sql.DataFrame, batchId : Long) => {\n",
    "        // Um batch pode contar várias ações duplicadas\n",
    "        val outputStream = microBatchOutputDF\n",
    "                                    .withColumn(\"row\",row_number().over(Window.partitionBy($\"symbol\").orderBy($\"last_trade_time\".desc)))\n",
    "                                    .where(\"row = 1\")\n",
    "                                    .drop(\"row\")\n",
    "        \n",
    "        deltaStock\n",
    "            .as(\"origem\")\n",
    "            .merge(\n",
    "                outputStream.as(\"att\"),\n",
    "                \"origem.symbol = att.symbol\"\n",
    "            )\n",
    "            .whenMatched().updateAll()\n",
    "            .whenNotMatched().insertAll()\n",
    "            .execute()\n",
    "        \n",
    "    }).start().awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printando Spark Dataframe como HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mRichDF\u001b[39m"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicit class RichDF(val ds: org.apache.spark.sql.DataFrame) {\n",
    "    def showHTML(limit:Int = 20, truncate: Int = 20) = {\n",
    "        import xml.Utility.escape\n",
    "        val data = ds.take(limit)\n",
    "        val header = ds.schema.fieldNames.toSeq        \n",
    "        val rows: Seq[Seq[String]] = data.map { row =>\n",
    "          row.toSeq.map { cell =>\n",
    "            val str = cell match {\n",
    "              case null => \"null\"\n",
    "              case binary: Array[Byte] => binary.map(\"%02X\".format(_)).mkString(\"[\", \" \", \"]\")\n",
    "              case array: Array[_] => array.mkString(\"[\", \", \", \"]\")\n",
    "              case seq: Seq[_] => seq.mkString(\"[\", \", \", \"]\")\n",
    "              case _ => cell.toString\n",
    "            }\n",
    "            if (truncate > 0 && str.length > truncate) {\n",
    "              // do not show ellipses for strings shorter than 4 characters.\n",
    "              if (truncate < 4) str.substring(0, truncate)\n",
    "              else str.substring(0, truncate - 3) + \"...\"\n",
    "            } else {\n",
    "              str\n",
    "            }\n",
    "          }: Seq[String]\n",
    "        }\n",
    "\n",
    "        publish.html(s\"\"\" <table>\n",
    "                <tr>\n",
    "                 ${header.map(h => s\"<th>${escape(h)}</th>\").mkString}\n",
    "                </tr>\n",
    "                ${rows.map { row =>\n",
    "                  s\"<tr>${row.map{c => s\"<td>${escape(c)}</td>\" }.mkString}</tr>\"\n",
    "                }.mkString}\n",
    "            </table>\n",
    "        \"\"\")        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.sql.AnalysisException: Failed to merge fields 'price' and 'price'. Failed to merge incompatible data types DoubleType and StringType;;\u001b[39m\n  org.apache.spark.sql.delta.schema.SchemaUtils$$anonfun$15.apply(\u001b[32mSchemaUtils.scala\u001b[39m:\u001b[32m526\u001b[39m)\n  org.apache.spark.sql.delta.schema.SchemaUtils$$anonfun$15.apply(\u001b[32mSchemaUtils.scala\u001b[39m:\u001b[32m515\u001b[39m)\n  scala.collection.TraversableLike$$anonfun$map$1.apply(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m234\u001b[39m)\n  scala.collection.TraversableLike$$anonfun$map$1.apply(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m234\u001b[39m)\n  scala.collection.IndexedSeqOptimized$class.foreach(\u001b[32mIndexedSeqOptimized.scala\u001b[39m:\u001b[32m33\u001b[39m)\n  scala.collection.mutable.ArrayOps$ofRef.foreach(\u001b[32mArrayOps.scala\u001b[39m:\u001b[32m186\u001b[39m)\n  scala.collection.TraversableLike$class.map(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m234\u001b[39m)\n  scala.collection.mutable.ArrayOps$ofRef.map(\u001b[32mArrayOps.scala\u001b[39m:\u001b[32m186\u001b[39m)\n  org.apache.spark.sql.delta.schema.SchemaUtils$.org$apache$spark$sql$delta$schema$SchemaUtils$$merge$1(\u001b[32mSchemaUtils.scala\u001b[39m:\u001b[32m515\u001b[39m)\n  org.apache.spark.sql.delta.schema.SchemaUtils$.mergeSchemas(\u001b[32mSchemaUtils.scala\u001b[39m:\u001b[32m591\u001b[39m)\n  org.apache.spark.sql.delta.schema.ImplicitMetadataOperation$class.updateMetadata(\u001b[32mImplicitMetadataOperation.scala\u001b[39m:\u001b[32m60\u001b[39m)\n  org.apache.spark.sql.delta.commands.WriteIntoDelta.updateMetadata(\u001b[32mWriteIntoDelta.scala\u001b[39m:\u001b[32m45\u001b[39m)\n  org.apache.spark.sql.delta.commands.WriteIntoDelta.write(\u001b[32mWriteIntoDelta.scala\u001b[39m:\u001b[32m84\u001b[39m)\n  org.apache.spark.sql.delta.commands.WriteIntoDelta$$anonfun$run$1.apply(\u001b[32mWriteIntoDelta.scala\u001b[39m:\u001b[32m65\u001b[39m)\n  org.apache.spark.sql.delta.commands.WriteIntoDelta$$anonfun$run$1.apply(\u001b[32mWriteIntoDelta.scala\u001b[39m:\u001b[32m64\u001b[39m)\n  org.apache.spark.sql.delta.DeltaLog.withNewTransaction(\u001b[32mDeltaLog.scala\u001b[39m:\u001b[32m386\u001b[39m)\n  org.apache.spark.sql.delta.commands.WriteIntoDelta.run(\u001b[32mWriteIntoDelta.scala\u001b[39m:\u001b[32m64\u001b[39m)\n  org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(\u001b[32mDeltaDataSource.scala\u001b[39m:\u001b[32m131\u001b[39m)\n  org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(\u001b[32mSaveIntoDataSourceCommand.scala\u001b[39m:\u001b[32m45\u001b[39m)\n  org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(\u001b[32mcommands.scala\u001b[39m:\u001b[32m70\u001b[39m)\n  org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(\u001b[32mcommands.scala\u001b[39m:\u001b[32m68\u001b[39m)\n  org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(\u001b[32mcommands.scala\u001b[39m:\u001b[32m86\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m80\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.toRdd(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m80\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m676\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m676\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m78\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m125\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m73\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.runCommand(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m676\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.saveToV1Source(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m285\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.save(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m271\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.save(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m229\u001b[39m)\n  ammonite.$sess.cmd61$Helper.<init>(\u001b[32mcmd61.sc\u001b[39m:\u001b[32m9\u001b[39m)\n  ammonite.$sess.cmd61$.<init>(\u001b[32mcmd61.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd61$.<clinit>(\u001b[32mcmd61.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "deltaStock\n",
    "    .toDF\n",
    "    .limit(1)\n",
    "    .withColumn(\"price\",$\"price\".cast(StringType))\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .save(stockDataPath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
